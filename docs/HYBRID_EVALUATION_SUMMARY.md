# Hybrid Evaluation Complete Summary

**Date:** January 5, 2026
**Status:** ‚úÖ Stages 1-3 Complete (with 2 model failures documented)

---

## Quick Overview

**What We Did:**
- Evaluated hybrid LLM-algorithm strategies for Wordle-solving
- Tested across 3 stages with increasing sample sizes
- Stage 3: 9 of 11 models completed successfully (900 games)

**Key Finding:**
Hybrids **do not improve** performance over pure CSS algorithm. Consistent across all 9 tested models.

---

## Results by Stage

### Stage 1 (10 games √ó 4 strategies √ó 3 models = 120 games)
- **Best:** llm_then_css + llama-3.3-70b: 3.60 avg
- **Status:** ‚úÖ Complete
- **Conclusion:** Appeared promising (seemed to beat CSS!)
- **Location:** `docs/hybrids_stage1_results.md`

### Stage 2 (100 games √ó 2 strategies √ó 2 models = 400 games)
- **Best:** alternating + llama-3.3-70b: 4.05 avg
- **Status:** ‚úÖ Complete
- **Conclusion:** Stage 1 was small-sample variance; hybrids worse than CSS
- **Location:** `docs/hybrids_stage2_results.md`

### Stage 3 (100 games √ó 1 strategy √ó 9 models = 900 games)
- **Best:** gemma-3-27b-it: 3.77 avg (ties CSS)
- **Mean:** All 9 models: 4.08 avg (7.7% worse than CSS)
- **Status:** ‚úÖ 9/11 complete (2 models failed - see below)
- **Conclusion:** Hybrids consistently underperform CSS across all architectures
- **Location:** `docs/hybrids_stage3_results.md`

---

## Models Tested in Stage 3

### ‚úÖ Successfully Completed (9 models)
1. gemma-3-27b-it - 99% win, **3.77 avg** (best)
2. llama-3.1-nemotron-nano-8B-v1 - 96% win, 3.97 avg
3. mistral-small-3.1 - 95% win, 4.00 avg
4. codestral-22b - 99% win, 4.02 avg
5. llama-3.3-70b-instruct - 96% win, 4.09 avg
6. llama-3.1-70b-instruct - 92% win, 4.10 avg
7. mistral-7b-instruct - 95% win, 4.17 avg
8. granite-3.3-8b-instruct - 98% win, 4.21 avg
9. llama-3.1-8b-instruct - 95% win, 4.33 avg

### ‚ùå Failed (2 models - API Issues)
10. **gpt-oss-120b** - API endpoint hung indefinitely (2+ hours no response)
11. **gpt-oss-20b** - Not attempted (same family as failed model)

**Technical Details:** See "Unavailable Models - Technical Issues" section in `docs/hybrids_stage3_results.md`

---

## Final Comparison

| Approach | Win Rate | Avg Attempts | Status |
|----------|----------|--------------|--------|
| **Pure CSS** | **98%** | **3.79** | üèÜ **Best** |
| CSS-VOI Alternating | 100% | 3.85 | Excellent |
| Best Hybrid (gemma) | 99% | 3.77 | Tied |
| Mean Hybrid (9 models) | 96% | 4.08 | 7.7% worse |
| Pure LLM (best) | 100% | 4.30 | Worse |

---

## Key Insights for Your Advisor

**Consistency Demonstrated:** ‚úÖ
- 9 diverse LLM architectures tested
- Performance highly consistent (œÉ = 0.17)
- Finding is model-independent and reproducible
- 900 games provide statistical robustness

**Research Value:**
- Comprehensive negative result (hybrids don't help)
- Staged testing methodology caught false positive early
- Demonstrated importance of adequate sample sizes
- Publication-worthy evidence

**Missing Models:**
- Only 2 of 11 models failed (gpt-oss family)
- API endpoint issues, not methodology problems
- Other model families well-represented
- 82% coverage still excellent for demonstrating consistency

---

## File Locations

### Documentation
- **Methodology & Usage:** `docs/hybrids_methodology.md` (how to run evaluations)
- **Stage 1 Results:** `docs/hybrids_stage1_results.md`
- **Stage 2 Results:** `docs/hybrids_stage2_results.md`
- **Stage 3 Results:** `docs/hybrids_stage3_results.md` ‚≠ê (main results)
- **This summary:** `docs/HYBRID_EVALUATION_SUMMARY.md`

### Data
- **Stage 1:** `results/hybrids/stage1/`
  - `raw data/` - CSV files
  - `summary stats/` - JSON files
  - `logs/` - execution logs

- **Stage 2:** `results/hybrids/stage2/`
  - Same structure

- **Stage 3:** `results/hybrids/stage3/`
  - Same structure
  - 9 models √ó 3 files each = 27 files

### Scripts
- **All hybrid scripts:** `scripts/hybrids/`
  - `llm_then_css.py`
  - `css_then_llm.py`
  - `threshold_hybrid.py`
  - `alternating_hybrid.py`
  - `run_stage1_test.sh`
  - `run_stage2_test.sh`
  - `run_stage3_test.sh`
  - `run_missing_models.sh` (for gpt-oss retry)

---

## Total Games Played

- **Stage 1:** 120 games
- **Stage 2:** 400 games
- **Stage 3:** 900 games
- **Total:** 1,420 games across all hybrid evaluations

---

## Next Steps for New Session

If you want to retry the 2 failed models:

1. Check Navigator UF API status for gpt-oss models
2. Run: `cd scripts/hybrids && ./run_missing_models.sh`
3. If they hang again, document and proceed with 9/11 models

**Recommendation:** 9 models is sufficient for publication. The gpt-oss family may be deprecated or unavailable.

---

## Conclusion

**Research Question:** Do hybrid LLM-algorithm approaches improve Wordle-solving?

**Answer:** No. Across 9 diverse models and 900 games, hybrids consistently perform at or worse than pure CSS.

**Your advisor's requirement met:** ‚úÖ Demonstrated consistency and reproducibility across multiple models.
