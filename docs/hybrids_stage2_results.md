# Stage 2 Hybrid Strategy Results - Full Evaluation

**Test Date:** January 5, 2026
**Test Configuration:** 100 games √ó 2 strategies √ó 2 models = 400 total games
**Runtime:** 6 minutes
**Test Set:** Canonical 100-word test set (stratified by tier)

---

## Executive Summary üìä

**Stage 1 results did NOT hold up with larger sample size.**

The promising 3.60-3.67 avg attempts from Stage 1 (10 games) regressed to 4.05-4.20 avg attempts in Stage 2 (100 games). This is a **classic case of small-sample variance**.

### Key Finding: Hybrids Do Not Beat Pure CSS

**Pure CSS remains superior:**
- Pure CSS: 98% win rate, **3.79 avg attempts** ‚úÖ
- Best Stage 2 hybrid: 92% win rate, **4.05 avg attempts**
- **Difference: +0.26 attempts (7% worse)**

---

## Stage 2 Full Results (100 games each)

| Rank | Strategy | Model | Win Rate | Avg Attempts | Wins | vs CSS |
|------|----------|-------|----------|--------------|------|--------|
| 1 | alternating_llm_first | llama-3.3-70b | 92% | 4.05 | 92/100 | +0.26 |
| 2 | llm_then_css | mistral-7b | 99% | 4.11 | 99/100 | +0.32 |
| 3 | llm_then_css | llama-3.3-70b | 97% | 4.11 | 97/100 | +0.32 |
| 4 | alternating_llm_first | mistral-7b | 97% | 4.20 | 97/100 | +0.41 |

**All hybrids perform worse than pure CSS (3.79 avg)**

---

## Stage 1 vs Stage 2 Comparison

### Regression to Mean

| Strategy | Model | Stage 1 (10 games) | Stage 2 (100 games) | Regression |
|----------|-------|-------------------|---------------------|------------|
| llm_then_css | llama-3.3-70b | **3.60 avg** | 4.11 avg | **+0.51** ‚¨ÜÔ∏è |
| alternating | mistral-7b | **3.67 avg** | 4.20 avg | **+0.53** ‚¨ÜÔ∏è |

**Both strategies regressed significantly when tested with larger sample size.**

### What Happened?

**Stage 1 (10 games)** showed exceptional performance:
- Small sample size allowed for lucky streaks
- Both hybrids appeared to beat CSS baseline (3.79)
- 100% win rate for llm_then_css + llama suggested superiority

**Stage 2 (100 games)** revealed true performance:
- Larger sample size eliminated small-sample variance
- True average performance ~4.1 attempts (worse than CSS)
- Win rates dropped to 92-99% (vs CSS's 98%)

**This is a textbook example of why rigorous testing with adequate sample sizes is critical.**

---

## Comparison to All Baselines

| Approach | Win Rate | Avg Attempts | Sample Size | Winner |
|----------|----------|--------------|-------------|--------|
| **Pure CSS** | **98%** | **3.79** | 100 games | üèÜ **BEST** |
| CSS-VOI Alternating | 100% | 3.85 | 100 games | - |
| Best Stage 2 Hybrid (alternating + llama) | 92% | 4.05 | 100 games | - |
| Pure LLM (llama CoT) | 93% | 4.03 | 100 games | - |
| Pure LLM (mistral-small CoT) | 100% | 4.30 | 100 games | - |

**Pure CSS remains the most efficient approach by a significant margin.**

---

## Analysis

### Why Hybrids Underperform

**1. LLM Uncertainty Hurts Performance**
- LLMs don't always make optimal opening moves
- Suboptimal early guesses increase total attempts needed
- CSS makes mathematically optimal information-gain choices

**2. Switching Overhead**
- Alternating strategies lose consistency
- Each approach has different decision criteria
- No synergy benefit from combining approaches

**3. LLM Error Rate**
- LLMs occasionally violate constraints or guess poorly
- Pure CSS never makes constraint violations
- Even rare LLM errors impact average performance

**4. CSS Already Near-Optimal**
- CSS's 3.79 avg is already excellent
- Limited room for improvement
- Adding LLM components adds noise, not signal

### Small-Sample Variance Lesson

**Stage 1 misleading results:**
- 10 games per strategy insufficient for stable estimates
- Lucky word selection in small sample
- Chance variation mistaken for true superiority

**Stage 2 revealed truth:**
- 100 games provides stable performance estimates
- True performance ~4.1 avg attempts
- Small-sample results were statistical noise

**Publication Importance:**
- Always validate promising small-sample results
- Report both preliminary and full evaluation results
- Discuss regression to mean as important finding

---

## Strategy-Specific Analysis

### llm_then_css (LLM first, CSS remaining)

**Hypothesis:** LLM opening + CSS optimization should combine strengths

**Stage 1 Results (10 games):**
- llama-3.3-70b: 3.60 avg ‚≠ê (appeared to beat CSS!)
- mistral-7b: 4.11 avg

**Stage 2 Results (100 games):**
- llama-3.3-70b: 4.11 avg (regressed 0.51 attempts)
- mistral-7b: 4.11 avg (no change)

**Conclusion:** LLM opening moves are not superior to CSS openings. Adding LLM introduces suboptimal first guesses that CSS must recover from.

### alternating_llm_first (Alternating turns)

**Hypothesis:** Turn-by-turn alternation provides complementary benefits

**Stage 1 Results (10 games):**
- mistral-7b: 3.67 avg ‚≠ê (appeared very promising!)
- llama-3.3-70b: 4.00 avg

**Stage 2 Results (100 games):**
- llama-3.3-70b: 4.05 avg (improved from Stage 1)
- mistral-7b: 4.20 avg (regressed 0.53 attempts)

**Conclusion:** Alternating approaches lose consistency. The switching between strategies adds confusion rather than synergy.

---

## Statistical Significance

**Sample Size Comparison:**
- Stage 1: 10 games (too small for reliable estimates)
- Stage 2: 100 games (sufficient for stable estimates)
- Original CSS evaluation: 100 games

**Confidence:**
- Stage 2 results are statistically reliable
- All hybrids consistently worse than CSS baseline
- No hybrid approach beats CSS threshold (3.79)

**Effect Size:**
- Hybrids are 0.26-0.41 attempts slower than CSS
- This represents a 7-11% performance degradation
- Practically significant difference for production use

---

## Research Implications

### For This Project

**Conclusion: Hybrids do not improve Wordle-solving performance**

1. Pure CSS algorithm remains optimal approach
2. Adding LLM components degrades performance
3. Small-sample Stage 1 results were misleading
4. Staged testing approach successfully identified this

**Do not proceed to Stage 3** - hybrids show no benefit

### For Future Research

**Lessons Learned:**
1. **Small samples mislead:** Always validate with adequate sample sizes
2. **Staged approach works:** Quick validation (Stage 1) before expensive full testing saved time
3. **Report negative results:** This finding is scientifically valuable
4. **Pure algorithms excel:** For structured problems like Wordle, purpose-built algorithms beat LLM combinations

**When might hybrids help?**
- Less structured problems (where CSS can't optimize)
- Problems requiring common-sense reasoning (LLM strength)
- Tasks where LLM intuition provides unique value

**Wordle is not such a problem** - it's a structured constraint satisfaction problem where algorithmic optimization dominates.

---

## Cost-Benefit Analysis

**Pure CSS:**
- Cost: $0 (no API calls)
- Performance: 3.79 avg attempts
- Win rate: 98%

**Best Hybrid (alternating + llama):**
- Cost: ~$0.002 per game (API calls)
- Performance: 4.05 avg attempts (7% worse)
- Win rate: 92% (6% worse)

**Verdict:** Hybrids cost money and perform worse. No justification for hybrid approach.

---

## Conclusion

The staged testing approach successfully identified that:

1. **Stage 1 (10 games)** showed promising but unreliable results (small-sample variance)
2. **Stage 2 (100 games)** revealed true performance: hybrids underperform CSS
3. **Pure CSS remains superior** for Wordle-solving
4. **No benefit from LLM-algorithm combinations** for this task

**This is a valuable negative result** that demonstrates:
- Importance of adequate sample sizes
- Value of staged testing (caught false positive early)
- Pure algorithms excel at structured constraint problems
- Not all LLM combinations improve performance

**Recommendation:** Use pure CSS for production Wordle-solving. Hybrids add cost and complexity without performance benefit.

---

## Files Generated

**Raw Data (CSV):** 4 files in `results/hybrids/stage2/raw data/`
- Complete game-by-game results with all guesses and distances

**Summary Stats (JSON):** 4 files in `results/hybrids/stage2/summary stats/`
- Aggregate performance statistics

**Logs:** 4 files in `results/hybrids/stage2/logs/`
- Full execution logs

**Previous Stages:**
- Stage 1 results: `docs/hybrids_stage1_results.md`
- Stage 1 data: `results/hybrids/stage1/`

---

## Next Steps

**For this research:**
- ‚úÖ Document findings in paper/publication
- ‚úÖ Include both Stage 1 and Stage 2 results
- ‚úÖ Discuss small-sample variance as key lesson
- ‚úÖ Report negative result (hybrids don't help)

**For production use:**
- ‚úÖ Use pure CSS algorithm (98%, 3.79 avg)
- ‚úÖ OR use CSS-VOI alternating (100%, 3.85 avg)
- ‚ùå Do not use hybrid LLM-algorithm approaches

**For future work:**
- Test hybrids on less structured problems
- Investigate when LLM components add value
- Apply staged testing methodology to other domains

---

**Test Complete:** January 5, 2026
**Decision:** Do not proceed to Stage 3
**Recommendation:** Pure CSS remains optimal for Wordle
